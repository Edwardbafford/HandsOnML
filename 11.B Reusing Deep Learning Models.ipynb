{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reusing Deep Learning Models\n",
    "\n",
    "Leverage TensorFlow tools to reuse lower layers of a deep nueral network and train a model on MNIST data (5-9) using minimal amounts of data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "sample_size = 500\n",
    "p = np.random.permutation(5 * sample_size)\n",
    "\n",
    "#Filter 5-9 for auxilary training\n",
    "x_train = np.concatenate((\n",
    "    x_train[y_train == 5][:sample_size],\n",
    "    x_train[y_train == 6][:sample_size],\n",
    "    x_train[y_train == 7][:sample_size],\n",
    "    x_train[y_train == 8][:sample_size],\n",
    "    x_train[y_train == 9][:sample_size]\n",
    "), axis=0)[p]\n",
    "\n",
    "y_train = np.concatenate((\n",
    "    y_train[y_train == 5][:sample_size],\n",
    "    y_train[y_train == 6][:sample_size],\n",
    "    y_train[y_train == 7][:sample_size],\n",
    "    y_train[y_train == 8][:sample_size],\n",
    "    y_train[y_train == 9][:sample_size]\n",
    "), axis=0)[p]\n",
    "\n",
    "x_test, y_test = x_test[y_test > 4], y_test[y_test > 4]\n",
    "\n",
    "#Clean\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train, x_test = x_train.reshape(-1,28*28), x_test.reshape(-1,28*28)\n",
    "y_train, y_test = y_train - 5, y_test - 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method for getting batches for training\n",
    "\n",
    "class mini_batches:\n",
    "    \n",
    "    def __init__(self, x, y, size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.size = size\n",
    "        self.index = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self.index + self.size >= len(self.x):            \n",
    "            batch_x = self.x[self.index:]\n",
    "            batch_y = self.y[self.index:]\n",
    "            self.index = 0\n",
    "            return batch_x, batch_y\n",
    "        \n",
    "        batch_x = self.x[self.index:self.index + self.size]\n",
    "        batch_y = self.y[self.index:self.index + self.size]\n",
    "        self.index = self.index + self.size\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the computational graph\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected \n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "# Inputs for training\n",
    "X = tf.placeholder(tf.float32, shape=(None,28*28), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "X_drop = dropout(X,.5, is_training=is_training)\n",
    "\n",
    "# Nueral Network layers\n",
    "with tf.name_scope('network'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    bn_params = {'is_training':is_training, 'decay':0.99, 'updates_collections':None}\n",
    "    \n",
    "    with tf.contrib.framework.arg_scope([fully_connected], weights_initializer=he_init, activation_fn=tf.nn.elu, \n",
    "                                        normalizer_fn=batch_norm, normalizer_params=bn_params):\n",
    "        h1 = dropout(fully_connected(X_drop, 100, scope='h1'))\n",
    "        h2 = dropout(fully_connected(h1, 100, scope='h2'))\n",
    "        h3 = dropout(fully_connected(h2, 100, scope='h3'))\n",
    "        h4 = dropout(fully_connected(h3, 100, scope='h4'))\n",
    "        h5 = dropout(fully_connected(h4, 100, scope='h5'))\n",
    "        output = fully_connected(h5, 5, scope='output', activation_fn=None)\n",
    "\n",
    "# Loss from Network\n",
    "with tf.name_scope('loss'):\n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    loss = tf.reduce_mean(x_entropy, name='loss')\n",
    "\n",
    "# SGD\n",
    "# NOTE: freeze lower layers at optimizer call\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train = optimizer.minimize(loss, var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='h[345]|output'))\n",
    "    \n",
    "# Evaluation of performance\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Map weights to current variables\n",
    "init = tf.global_variables_initializer()\n",
    "\n",
    "value_list = []\n",
    "value_list.extend(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='h[12]'))\n",
    "\n",
    "og_saver = tf.train.Saver(value_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tensorflow/models/11_deep_learning.ckpt\n",
      "0.21826784\n",
      "0.4929027\n",
      "0.5671672\n",
      "0.6056367\n",
      "0.6218885\n",
      "0.6295001\n",
      "0.6336145\n",
      "0.6406089\n",
      "0.6533635\n",
      "0.64945483\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Mini batches\n",
    "batches = mini_batches(x_train, y_train, sample_size)\n",
    "max_acc = 0\n",
    "epochs = 0\n",
    "\n",
    "# For saving model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Log files\n",
    "import os\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "log_dir = os.path.join(os.getcwd(), 'tensorflow/logs/11-reuse-model-{}/'.format(now))\n",
    "acc_summary = tf.summary.scalar('11_reuse_model_accuracy',accuracy)\n",
    "writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    #Restore Weights from old model\n",
    "    og_saver.restore(sess,'./tensorflow/models/11_deep_learning.ckpt')\n",
    "    \n",
    "    # SGD Updates\n",
    "    for index, batch in enumerate(range(sample_size)):\n",
    "        batch_x, batch_y = batches.next_batch()\n",
    "        sess.run(train, feed_dict={X: batch_x, y:batch_y, is_training:True})\n",
    "        \n",
    "        # Early stopping and Checkpoint logging\n",
    "        if index % 50 == 0:\n",
    "            saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_reuse_model.ckpt'))\n",
    "            log_str = acc_summary.eval(feed_dict={X: x_test, y:y_test, is_training:False})\n",
    "            writer.add_summary(log_str, index)\n",
    "            \n",
    "            cur_acc = accuracy.eval(feed_dict={X: x_test, y:y_test, is_training:False})\n",
    "            print(cur_acc)\n",
    "            if cur_acc > max_acc:\n",
    "                max_acc = cur_acc\n",
    "                epochs = 0\n",
    "            else:\n",
    "                epochs = epochs + 1\n",
    "                if epochs > 3:\n",
    "                    saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_reuse_model.ckpt'))\n",
    "                    break\n",
    "\n",
    "    # Save final model\n",
    "    saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_reuse_model.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pretty meh... how do we get the network to abstract better at lower layers.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
