{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Auxiliary Learning \n",
    "\n",
    "Train a tensorflow network to learn to recognize if two MNIST digits are the same. Then retrain on minimal data to recognize all nine digits!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "p = np.random.permutation(len(y_train))\n",
    "\n",
    "x_reg, y_reg = x_train[p][:5000], y_train[p][:5000]\n",
    "x_aux, y_aux = x_train[p][5000:], y_train[p][5000:]\n",
    "\n",
    "x_reg, x_aux, x_test = x_reg/255.0, x_aux/255.0, x_test/255.0\n",
    "x_reg, x_aux, x_test = x_reg.reshape(-1,28*28), x_aux.reshape(-1,28*28), x_test.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Auxiliary Graph\n",
    "Build and train the auxiliary graph. Use its layers to build a new model!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function for getting mini batches of matching and non matching images\n",
    "\n",
    "def aux_mini_batch(x_aux, y_aux, size):\n",
    "    p1 = np.random.permutation(len(y_aux))\n",
    "    p2 = np.random.permutation(len(y_aux))\n",
    "    \n",
    "    s_left = x_aux[p1][y_aux[p1] == y_aux[p2]][:size]\n",
    "    s_right = x_aux[p2][y_aux[p1] == y_aux[p2]][:size]\n",
    "    same = np.ones(size)\n",
    "    \n",
    "    d_left = x_aux[p1][y_aux[p1] != y_aux[p2]][:size]\n",
    "    d_right = x_aux[p2][y_aux[p1] != y_aux[p2]][:size]\n",
    "    diff = np.zeros(size)\n",
    "\n",
    "    left = np.concatenate((s_left, d_left), axis=0)\n",
    "    right = np.concatenate((s_right, d_right), axis=0)\n",
    "    labels = np.concatenate((same, diff))\n",
    "    \n",
    "    pf = np.random.permutation(len(labels))\n",
    "    \n",
    "    return left[pf], right[pf], labels[pf]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Aux computational graph\n",
    "# Two networks, each one takes an image and matches i\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected \n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "# Inputs for training\n",
    "left = tf.placeholder(tf.float32, shape=(None,28*28), name='X')\n",
    "right = tf.placeholder(tf.float32, shape=(None,28*28), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "\n",
    "left_drop = dropout(left,.5, is_training=is_training)\n",
    "right_drop = dropout(right,.5, is_training=is_training)\n",
    "\n",
    "# Nueral Network layers\n",
    "with tf.name_scope('network'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    bn_params = {'is_training':is_training, 'decay':0.99, 'updates_collections':None}\n",
    "    \n",
    "    # Left side of NN\n",
    "    with tf.name_scope('left_network'):\n",
    "        with tf.contrib.framework.arg_scope([fully_connected], weights_initializer=he_init, activation_fn=tf.nn.elu, \n",
    "                                        normalizer_fn=batch_norm, normalizer_params=bn_params):\n",
    "            l1 = dropout(fully_connected(left_drop, 100, scope='l1'))\n",
    "            l2 = dropout(fully_connected(l1, 100, scope='l2'))\n",
    "            l3 = dropout(fully_connected(l2, 100, scope='l3'))\n",
    "            l4 = dropout(fully_connected(l3, 100, scope='l4'))\n",
    "            l5 = dropout(fully_connected(l4, 100, scope='l5'))\n",
    "    \n",
    "    # Right side of NN\n",
    "    with tf.name_scope('right_network'):\n",
    "        with tf.contrib.framework.arg_scope([fully_connected], weights_initializer=he_init, activation_fn=tf.nn.elu, \n",
    "                                        normalizer_fn=batch_norm, normalizer_params=bn_params):\n",
    "            r1 = dropout(fully_connected(right_drop, 100, scope='r1'))\n",
    "            r2 = dropout(fully_connected(r1, 100, scope='r2'))\n",
    "            r3 = dropout(fully_connected(r2, 100, scope='r3'))\n",
    "            r4 = dropout(fully_connected(r3, 100, scope='r4'))\n",
    "            r5 = dropout(fully_connected(r4, 100, scope='r5'))\n",
    "    \n",
    "    top = tf.concat([l5,r5], axis=1, name='top')\n",
    "    output = fully_connected(top, 1, scope='output', activation_fn=tf.nn.sigmoid)\n",
    "\n",
    "# Loss from Network\n",
    "# Equivalent to performance\n",
    "with tf.name_scope('loss'):\n",
    "    loss = tf.losses.mean_squared_error(labels=y, predictions=output)\n",
    "\n",
    "# SGD\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train = optimizer.minimize(loss)\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.41190735\n",
      "0.40104935\n",
      "0.39424938\n",
      "0.39006904\n",
      "0.3798521\n",
      "0.3758725\n",
      "0.3690762\n",
      "0.3643566\n",
      "0.35875094\n",
      "0.35261756\n",
      "0.3475057\n",
      "0.3392602\n",
      "0.33750314\n",
      "0.33381855\n",
      "0.32999775\n",
      "0.3236753\n",
      "0.32369298\n",
      "0.32036403\n",
      "0.3175712\n",
      "0.31400186\n",
      "0.31197724\n",
      "0.30757162\n",
      "0.30443674\n",
      "0.29822594\n",
      "0.29896343\n",
      "0.29569244\n",
      "0.2943005\n",
      "0.29203767\n",
      "0.2900126\n",
      "0.28767797\n",
      "0.2850445\n",
      "0.28298783\n",
      "0.28146452\n",
      "0.27855822\n",
      "0.27762994\n",
      "0.27572697\n",
      "0.27355176\n",
      "0.27378914\n",
      "0.27139482\n",
      "0.2715755\n",
      "0.26874173\n",
      "0.26764333\n",
      "0.26627398\n",
      "0.2660837\n",
      "0.26478156\n",
      "0.26381898\n",
      "0.26254287\n",
      "0.2617081\n",
      "0.26137707\n",
      "0.26064038\n",
      "0.2598513\n",
      "0.25861531\n",
      "0.25847474\n",
      "0.2579613\n",
      "0.25740445\n",
      "0.2572683\n",
      "0.25644597\n",
      "0.2558217\n",
      "0.25560394\n",
      "0.25504237\n",
      "0.25468007\n",
      "0.25469816\n",
      "0.25420633\n",
      "0.2538021\n",
      "0.2535938\n",
      "0.25332958\n",
      "0.25309005\n",
      "0.25283602\n",
      "0.25272945\n",
      "0.25236842\n",
      "0.2522171\n",
      "0.25215274\n",
      "0.25187445\n",
      "0.25169408\n",
      "0.251594\n",
      "0.25154728\n",
      "0.25136232\n",
      "0.2512448\n",
      "0.25118205\n",
      "0.25109467\n",
      "0.25102684\n",
      "0.2509364\n",
      "0.2508367\n",
      "0.25070387\n",
      "0.25066844\n",
      "0.25063768\n",
      "0.2506633\n",
      "0.2505549\n",
      "0.25049782\n",
      "0.25046346\n",
      "0.2504297\n",
      "0.250388\n",
      "0.25033915\n",
      "0.2503365\n",
      "0.2502766\n",
      "0.25027102\n",
      "0.25024822\n",
      "0.25022444\n",
      "0.2502109\n",
      "0.250179\n",
      "0.25017697\n",
      "0.25016317\n",
      "0.25015098\n",
      "0.2501258\n",
      "0.25011712\n",
      "0.25010404\n",
      "0.25010315\n",
      "0.2500858\n",
      "0.2500831\n",
      "0.25007716\n",
      "0.2500644\n",
      "0.2500611\n",
      "0.25005624\n",
      "0.25006223\n",
      "0.2500455\n",
      "0.25003973\n",
      "0.25003636\n",
      "0.25003245\n",
      "0.25003457\n",
      "0.25002876\n",
      "0.25002563\n",
      "0.2500261\n",
      "0.25002518\n",
      "0.25001806\n",
      "0.2500192\n",
      "0.25001532\n",
      "0.25001338\n",
      "0.25000936\n",
      "0.25000924\n",
      "0.25001034\n",
      "0.2500131\n",
      "0.2500105\n",
      "0.2500057\n",
      "0.25000843\n",
      "0.2500079\n",
      "0.25000504\n",
      "0.2500049\n",
      "0.25000715\n",
      "0.25000474\n",
      "0.2500047\n",
      "0.25000212\n",
      "0.25000286\n",
      "0.25000295\n",
      "0.25000352\n",
      "0.25000274\n",
      "0.25000274\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "max_loss = np.inf\n",
    "epochs = 0\n",
    "\n",
    "# Save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Log files\n",
    "import os\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "log_dir = os.path.join(os.getcwd(), 'tensorflow/logs/11-aux-learning-{}/'.format(now))\n",
    "mse_summary = tf.summary.scalar('11_MSE_aux_learning',loss)\n",
    "writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    # SGD Updates\n",
    "    for index, batch in enumerate(range(2000)):\n",
    "        l_batch, r_batch, y_batch = aux_mini_batch(x_aux, y_aux, 1000)\n",
    "        sess.run(train, feed_dict={left: l_batch, right:r_batch, y:y_batch, is_training:True})\n",
    "        \n",
    "        # Early stopping Logging and Checkpoint Saving\n",
    "        if index % 2 == 0:\n",
    "            saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_aux_learning.ckpt'))\n",
    "            log_str = mse_summary.eval(feed_dict={left: l_batch, right:r_batch, y:y_batch, is_training:False})\n",
    "            writer.add_summary(log_str, index)\n",
    "            \n",
    "            cur_loss = loss.eval(feed_dict={left: l_batch, right:r_batch, y:y_batch, is_training:False})\n",
    "            print(cur_loss)\n",
    "            if cur_loss < max_loss:\n",
    "                max_loss = cur_loss\n",
    "                epochs = 0\n",
    "            else:\n",
    "                epochs = epochs + 1\n",
    "                if epochs == 5:\n",
    "                    saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_aux_learning.ckpt'))\n",
    "                    break\n",
    "\n",
    "    # Save final model\n",
    "    saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_aux_learning.ckpt'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Supported Graph\n",
    "Reuse layers of trained auxiliary network to quickly train our own MNIST classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data\n",
    "\n",
    "mnist = tf.keras.datasets.mnist\n",
    "(x_train, y_train),(x_test, y_test) = mnist.load_data()\n",
    "\n",
    "#Filter 0-4 for auxilary training in later notebook\n",
    "x_train, y_train = x_train[y_train <= 4], y_train[y_train <= 4]\n",
    "x_test, y_test = x_test[y_test <= 4], y_test[y_test <= 4]\n",
    "\n",
    "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
    "x_train, x_test = x_train.reshape(-1,28*28), x_test.reshape(-1,28*28)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create method for getting batches for training\n",
    "\n",
    "class mini_batches:\n",
    "    \n",
    "    def __init__(self, x, y, size):\n",
    "        self.x = x\n",
    "        self.y = y\n",
    "        self.size = size\n",
    "        self.index = 0\n",
    "    \n",
    "    def next_batch(self):\n",
    "        if self.index + self.size >= len(self.x):            \n",
    "            batch_x = self.x[self.index:]\n",
    "            batch_y = self.y[self.index:]\n",
    "            self.index = 0\n",
    "            return batch_x, batch_y\n",
    "        \n",
    "        batch_x = self.x[self.index:self.index + self.size]\n",
    "        batch_y = self.y[self.index:self.index + self.size]\n",
    "        self.index = self.index + self.size\n",
    "        return batch_x, batch_y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Build the computational graph\n",
    "\n",
    "from tensorflow.contrib.layers import fully_connected \n",
    "from tensorflow.contrib.layers import batch_norm\n",
    "from tensorflow.contrib.layers import dropout\n",
    "\n",
    "tf.reset_default_graph()\n",
    "\n",
    "\n",
    "is_training = tf.placeholder(tf.bool, shape=(), name='is_training')\n",
    "\n",
    "# Inputs for training\n",
    "X = tf.placeholder(tf.float32, shape=(None,28*28), name='X')\n",
    "y = tf.placeholder(tf.int32, shape=(None), name='y')\n",
    "X_drop = dropout(X,.5, is_training=is_training)\n",
    "\n",
    "# Nueral Network layers\n",
    "with tf.name_scope('network'):\n",
    "    he_init = tf.contrib.layers.variance_scaling_initializer()\n",
    "    bn_params = {'is_training':is_training, 'decay':0.99, 'updates_collections':None}\n",
    "    \n",
    "    with tf.contrib.framework.arg_scope([fully_connected], weights_initializer=he_init, activation_fn=tf.nn.elu, \n",
    "                                        normalizer_fn=batch_norm, normalizer_params=bn_params):\n",
    "        h1 = dropout(fully_connected(X_drop, 100, scope='l1'))\n",
    "        h2 = dropout(fully_connected(h1, 100, scope='l2'))\n",
    "        h3 = dropout(fully_connected(h2, 100, scope='l3'))\n",
    "        h4 = dropout(fully_connected(h3, 100, scope='l4'))\n",
    "        h5 = dropout(fully_connected(h4, 100, scope='l5'))\n",
    "        output = fully_connected(h5, 10, scope='output', activation_fn=None)\n",
    "\n",
    "# Loss from Network\n",
    "with tf.name_scope('loss'):\n",
    "    x_entropy = tf.nn.sparse_softmax_cross_entropy_with_logits(labels=y, logits=output)\n",
    "    loss = tf.reduce_mean(x_entropy, name='loss')\n",
    "\n",
    "# SGD\n",
    "with tf.name_scope('train'):\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    train = optimizer.minimize(loss, var_list=tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='l[345]|output'))\n",
    "    \n",
    "# Evaluation of performance\n",
    "with tf.name_scope('eval'):\n",
    "    correct = tf.nn.in_top_k(output, y, 1)\n",
    "    accuracy = tf.reduce_mean(tf.cast(correct, tf.float32))\n",
    "    \n",
    "init = tf.global_variables_initializer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<tf.Variable 'l1/weights:0' shape=(784, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'l1/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'l1/BatchNorm/moving_mean:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'l1/BatchNorm/moving_variance:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'l2/weights:0' shape=(100, 100) dtype=float32_ref>,\n",
       " <tf.Variable 'l2/BatchNorm/beta:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'l2/BatchNorm/moving_mean:0' shape=(100,) dtype=float32_ref>,\n",
       " <tf.Variable 'l2/BatchNorm/moving_variance:0' shape=(100,) dtype=float32_ref>]"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Map weights to current variables\n",
    "value_list = []\n",
    "value_list.extend(tf.get_collection(tf.GraphKeys.GLOBAL_VARIABLES, scope='l[12]'))\n",
    "\n",
    "og_saver = tf.train.Saver(value_list)\n",
    "\n",
    "value_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:tensorflow:Restoring parameters from ./tensorflow/models/11_aux_learning.ckpt\n",
      "0.103911266\n",
      "0.769605\n",
      "0.7742751\n",
      "0.78361547\n",
      "0.7853668\n",
      "0.7853668\n",
      "0.7838101\n",
      "0.78906405\n",
      "0.7931504\n",
      "0.78886944\n",
      "0.7915937\n",
      "0.7929558\n",
      "0.7896478\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "\n",
    "# Mini batches\n",
    "batches = mini_batches(x_train, y_train, 1000)\n",
    "max_acc = 0\n",
    "epochs = 0\n",
    "\n",
    "# Save model\n",
    "saver = tf.train.Saver()\n",
    "\n",
    "# Log files\n",
    "import os\n",
    "from datetime import datetime\n",
    "now = datetime.utcnow().strftime('%Y%m%d%H%M%S')\n",
    "log_dir = os.path.join(os.getcwd(), 'tensorflow/logs/11-axu-reuse-learning-{}/'.format(now))\n",
    "acc_summary = tf.summary.scalar('11_aux_reuse_acc',accuracy)\n",
    "writer = tf.summary.FileWriter(log_dir, tf.get_default_graph())\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    init.run()\n",
    "    \n",
    "    #Restore Weights from old model\n",
    "    og_saver.restore(sess,'./tensorflow/models/11_aux_learning.ckpt')\n",
    "    \n",
    "    # SGD Updates\n",
    "    for index, batch in enumerate(range(50000)):\n",
    "        batch_x, batch_y = batches.next_batch()\n",
    "        sess.run(train, feed_dict={X: batch_x, y:batch_y, is_training:True})\n",
    "        \n",
    "        # Early stopping and Checkpoint logging\n",
    "        if index % 1000 == 0:\n",
    "            saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_aux_reuse_learning.ckpt'))\n",
    "            log_str = acc_summary.eval(feed_dict={X: x_test, y:y_test, is_training:False})\n",
    "            writer.add_summary(log_str, index)\n",
    "            \n",
    "            cur_acc = accuracy.eval(feed_dict={X: x_test, y:y_test, is_training:False})\n",
    "            print(cur_acc)\n",
    "            if cur_acc > max_acc:\n",
    "                max_acc = cur_acc\n",
    "                epochs = 0\n",
    "            else:\n",
    "                epochs = epochs + 1\n",
    "                if epochs > 3:\n",
    "                    saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_aux_reuse_learning.ckpt'))\n",
    "                    break\n",
    "\n",
    "    # Save final model\n",
    "    saver.save(sess, os.path.join(os.getcwd(), 'tensorflow/models/11_aux_reuse_learning.ckpt'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
